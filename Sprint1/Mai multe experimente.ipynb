{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4960728300452282736]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The machine settings are listed.\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the activated GPU device on remote machine\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impoooooortam tot ce trebuie :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "import csv\n",
    "with open('test_dataset.csv', encoding=\"utf8\") as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        tweets.append(row);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3044"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook Roundup: #LondonsSchoolSucks London_ Pages_ School_ Spam_ Foursquare @AmyBello @Sharmdja... http://bit.ly/p2yedk or  http://bit.ly/p2yXBn'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_list=[]\n",
    "tweets_label_list=[]\n",
    "for row in tweets:\n",
    "    tweets_text_list.append(row[1])\n",
    "    tweets_label_list.append(row[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text_list\n",
    "del tweets_text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '',\n",
       " '1',\n",
       " '',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " ...]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tweets_label_list[0]\n",
    "tweets_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets :  3043\n",
      "class 0:  251\n",
      "class 1:  2812\n",
      "fara label:  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets : \",len(tweets_label_list))\n",
    "value1 = [i for i in tweets_label_list if i in '0']\n",
    "value2 = [i for i in tweets_label_list if i in '1']\n",
    "value3 = [i for i in tweets_label_list if i in '']\n",
    "print(\"class 0: \",len(value1))\n",
    "print(\"class 1: \",len(value2))\n",
    "print(\"fara label: \", len(value3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_list=[]\n",
    "aux_label_list=[]\n",
    "unlabeled_tweets=[]\n",
    "#Trebuie sa eliminaaam tweet urile fara label\n",
    "for i in range(len(tweets_text_list)):\n",
    "    if tweets_label_list[i]!='':\n",
    "        aux_list.append(tweets_text_list[i])\n",
    "        aux_label_list.append(tweets_label_list[i])\n",
    "    else:\n",
    "        unlabeled_tweets.append(tweets_text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cleanup efforts in Clapham_ the people of London are quickly rebuilding and restoring lives. Such a great photo! http://fb.me/1amol0TJb',\n",
       " '#london #jobs Senior Health Economist - Pharma: Location: Bracknell BerkshireSalary: 50000-59000',\n",
       " 'Freelance: Social Media Executive (Freelance - 8 Weeks) -  London: Social Media Executive (Freelance - 8 Weeks)',\n",
       " 'Book a flight to London. ',\n",
       " '@cc_1224 I remember when you came to London_ I was close but in meetings. But you enjoyed our city and will again. :-)',\n",
       " 'RT : Blazes_ looting in London riot #police #shooting #savePeople #spreadNews ',\n",
       " 'Blazes_ looting in London riot #police #shooting #savePeople #spreadNews ',\n",
       " 'London massive lootings explained as unrest caused by \"social exclusion\". Crime on a massive scale is still crime.  ',\n",
       " 'London massive lootings explained as unrest caused by \"social exclusion\". Crime on a massive scale is still crime.  ',\n",
       " 'Blazes_ looting in London riot #police #shooting #savePeople #spreadNews ',\n",
       " 'Blazes_ looting in London riot #police #shooting #savePeople #spreadNews ',\n",
       " 'London Riots. (The BBC will never replay this. Send it out)',\n",
       " '#London Riots. (The BBC will never replay this. Send it out)  http://bit.ly/pEAgR',\n",
       " 'London City Soccer (@LondonCity1973)',\n",
       " 'EMI bids range from $3 bln to $4 bln-FT: LONDON_ Aug 3 (Reuters) - Bids for EMI [LNDONE.UL] suggest',\n",
       " 'Disturbance after London protest - http://www.bbc.co.uk/news/uk-england-london-14434318.   #updates #riots',\n",
       " '#oneyeartogo',\n",
       " '#oneyeartogo',\n",
       " '(Triathlon)Photos: Pre-Race Activities In London',\n",
       " '(Triathlon)Photos: Pre-Race Activities In London']"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_text_list=aux_list\n",
    "tweets_label_list=aux_label_list\n",
    "unlabeled_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets :  3023\n",
      "class 0:  231\n",
      "class 1:  2792\n",
      "fara label:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets : \",len(tweets_label_list))\n",
    "value1 = [i for i in tweets_label_list if i in '0']\n",
    "value2 = [i for i in tweets_label_list if i in '1']\n",
    "value3 = [i for i in tweets_label_list if i in '']\n",
    "print(\"class 0: \",len(value1))\n",
    "print(\"class 1: \",len(value2))\n",
    "print(\"fara label: \", len(value3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesaaaaare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "\n",
    "\n",
    "def splitIntoStem(message):\n",
    "\n",
    "    \n",
    "    return [removeNumeric(stripEmoji(singleCharacterRemove(removePunctuation\n",
    "                                                           (removeHyperlinks\n",
    "                                                            (removeHashtags\n",
    "                                                             (removeUsernames\n",
    "                                                              (stemWord(word)))))))) for word in message.split()]\n",
    "def stemWord(tweet):\n",
    "\n",
    "    ps = PorterStemmer()                       \n",
    "    return ps.stem(tweet).lower()\n",
    "    \n",
    "\n",
    "#Remove usernames\n",
    "def removeUsernames(tweet):\n",
    "    return re.sub('@[^\\s]+', '', tweet)\n",
    "\n",
    "\n",
    "#Remove hashtag\n",
    "def removeHashtags(tweet):\n",
    "    return re.sub(r'#[^\\s]+', '', tweet)\n",
    "\n",
    "#Remove link\n",
    "def removeHyperlinks(tweet):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
    "\n",
    "#Remove numeric character\n",
    "def removeNumeric(value):\n",
    "    blist2 = [item for item in value if not item.isdigit()]\n",
    "    blist3 = \"\".join(blist2)\n",
    "    return blist3\n",
    "\n",
    "#Remove punctuation\n",
    "def removePunctuation(tweet):\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]','',tweet)\n",
    "\n",
    "#Remove single character\n",
    "def singleCharacterRemove(tweet):\n",
    "    tweet=tweet.replace('_', '')\n",
    "    return re.sub(r'(?:^| )\\w(?:$| \\ )', ' ', tweet)\n",
    "\n",
    "#Remove emoji\n",
    "def stripEmoji(text):\n",
    "\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets=[]\n",
    "unlabeled_filtered_tweets=[]\n",
    "for i,k in enumerate(tweets_text_list):\n",
    "    filtered_tweets.append(\" \".join(splitIntoStem(k)).split())\n",
    "for i,k in enumerate(unlabeled_tweets):\n",
    "    unlabeled_filtered_tweets.append(\" \".join(splitIntoStem(k)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facebook', 'roundup', 'london', 'pages', 'school', 'spam', 'foursquar', 'or']"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook Roundup: #LondonsSchoolSucks London_ Pages_ School_ Spam_ Foursquare @AmyBello @Sharmdja... http://bit.ly/p2yedk or  http://bit.ly/p2yXBn'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleanup',\n",
       " 'effort',\n",
       " 'in',\n",
       " 'clapham',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'of',\n",
       " 'london',\n",
       " 'are',\n",
       " 'quickli',\n",
       " 'rebuild',\n",
       " 'and',\n",
       " 'restor',\n",
       " 'lives',\n",
       " 'such',\n",
       " 'great',\n",
       " 'photo']"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_filtered_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Roxanica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminaaam stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def removeStopWords(tweet_list):\n",
    "    \n",
    "    filtered_stopwords = []\n",
    "    filtered_stopwords_list = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    print(\"stop_words : \",stop_words)\n",
    "    \n",
    "    for i in filtered_tweets:\n",
    "        filtered_sentence = [w for w in i if not w in stop_words]\n",
    "        filtered_stopwords_list.append(filtered_sentence)                         #return list value\n",
    "        filtered_stopwords.append(\" \".join(filtered_sentence))                    #return string value\n",
    "    \n",
    "    return filtered_stopwords,filtered_stopwords_list                                      \n",
    "\n",
    "\n",
    "filtered_tweets, filtered_stopwords_list = removeStopWords(filtered_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook roundup london pages school spam foursquar'"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleanup',\n",
       " 'effort',\n",
       " 'in',\n",
       " 'clapham',\n",
       " 'the',\n",
       " 'peopl',\n",
       " 'of',\n",
       " 'london',\n",
       " 'are',\n",
       " 'quickli',\n",
       " 'rebuild',\n",
       " 'and',\n",
       " 'restor',\n",
       " 'lives',\n",
       " 'such',\n",
       " 'great',\n",
       " 'photo']"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_filtered_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def removeStopWords_u(tweet_list):\n",
    "    \n",
    "    filtered_stopwords_u = []\n",
    "    filtered_stopwords_list_u = []\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    print(\"stop_words : \",stop_words)\n",
    "    \n",
    "    for i in unlabeled_filtered_tweets:\n",
    "        filtered_sentence = [w for w in i if not w in stop_words]\n",
    "        filtered_stopwords_list_u.append(filtered_sentence)                         #return list value\n",
    "        filtered_stopwords_u.append(\" \".join(filtered_sentence))                    #return string value\n",
    "    \n",
    "    return filtered_stopwords_u,filtered_stopwords_list_u                                      \n",
    "\n",
    "\n",
    "filtered_unlabeled_tweets, filtered_stopwords_list = removeStopWords_u(unlabeled_filtered_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleanup effort clapham peopl london quickli rebuild restor lives great photo',\n",
       " 'senior health economist pharma location bracknel berkshiresalary',\n",
       " 'freelance social media execut freelanc weeks london social media execut freelanc weeks',\n",
       " 'book flight london',\n",
       " 'rememb came london wa close meetings enjoy citi',\n",
       " 'rt blazes loot london riot',\n",
       " 'blazes loot london riot',\n",
       " 'london massiv loot explain unrest caus social exclusion crime massiv scale still crime',\n",
       " 'london massiv loot explain unrest caus social exclusion crime massiv scale still crime',\n",
       " 'blazes loot london riot',\n",
       " 'blazes loot london riot',\n",
       " 'london riots bbc never replay send',\n",
       " 'riots bbc never replay send',\n",
       " 'london citi soccer',\n",
       " 'emi bid rang bln blnft london aug reuters bid emi lndoneul suggest',\n",
       " 'disturb london protest',\n",
       " '',\n",
       " 'triathlonphotos prerac activ london',\n",
       " 'triathlonphotos prerac activ london']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_unlabeled_tweets.remove('')\n",
    "filtered_unlabeled_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook Roundup: #LondonsSchoolSucks London_ Pages_ School_ Spam_ Foursquare @AmyBello @Sharmdja... http://bit.ly/p2yedk or  http://bit.ly/p2yXBn'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook roundup london pages school spam four...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook roundup pages school spam foursquar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt new guidelin product brand london olymp bik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt new guidelin product brand london olymp bik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>london brand new appl macbook pro london price â</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "0  facebook roundup london pages school spam four...     0\n",
       "1       facebook roundup pages school spam foursquar     0\n",
       "2  rt new guidelin product brand london olymp bik...     1\n",
       "3  rt new guidelin product brand london olymp bik...     1\n",
       "4   london brand new appl macbook pro london price â     1"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"tweet\":filtered_tweets,\"label\":tweets_label_list})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet    facebook roundup london pages school spam four...\n",
       "label                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['label'].values.tolist()\n",
    "data = df['tweet'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(data, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : 2418\n",
      "x_test : 605\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train :\",len(x_train))\n",
    "print(\"x_test :\",len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfl transport london mayor transport organisation much loved londoners'"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x288c4f6b688>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()             #Tokenizer(num_words=5000) => 5000 words of the highest frequency\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['london', 'checkzz', 'rt', 'riot', 'link', 'bio', 'go', 'im', 'thi', 'polic']"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.word_index)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer) : 6440\n"
     ]
    }
   ],
   "source": [
    "print(\"len(tokenizer) :\",len(list(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set token with text_to_sequences.\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfl transport london mayor transport organisation much loved londoners'"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4673, 239, 1, 397, 239, 4674, 83, 4675, 4676]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perbuatan biadab disela kerusuhan london bio link indonesia inforoot soccer leagu cup game cancel due london riots due larg number riot occur bio link mullins inforoot riot london whaaat someon care explain inforoot al je mn tele wilt moet je voor donderdag komen halen want donderdag ga ik miss naar londonvolg inforoot need come londonmichael day inforoot got work tomoro hav sum friend comin london stay us quit excit altho wont twitter muchlillie inforoot oh londonudahlah jgn rusuhtar premier leagu keganggugw ga ada tontonan bola soalnyakompetisi pssi ramean kepengurusan bukan liganyarully inforoot hey big support ministri londonjust show love aw wow thank muchth wall group inforoot rememb movi ghosts went music night londonlov itim theatr junkielov dem ting deh inforoot stay safe ur london wednesday ikk worri lad days xxxbonnie inforoot wait olymp come thi london imagin lool'"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[np.argmax(num_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824677472709229"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many of the eighteen length tweets are included?\n",
    "\n",
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tokens : [127, 60, 129, 1012, 959, 1012, 4284, 127, 60, 4285, 2]\n",
      "x_train_pad : [   0    0    0    0    0    0    0  127   60  129 1012  959 1012 4284\n",
      "  127   60 4285    2]\n"
     ]
    }
   ],
   "source": [
    "#Zero is added before the values given in the padding operation.\n",
    "\n",
    "print(\"x_train_tokens :\",x_train_tokens[0])\n",
    "print(\"x_train_pad :\",x_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_tokens : [105, 2518, 6270, 1, 9, 92, 168, 33, 2521, 1239]\n",
      "x_train_pad : [   0    0    0    0    0    0    0    0  105 2518 6270    1    9   92\n",
      "  168   33 2521 1239]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test_tokens :\",x_test_tokens[0])\n",
    "print(\"x_train_pad :\",x_test_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_pad.shape : (2418, 18)\n",
      "x_train_pad.shape : (605, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train_pad.shape :\",x_train_pad.shape)\n",
    "print(\"x_train_pad.shape :\",x_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfl transport london mayor transport organisation much loved londoners'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4673, 239, 1, 397, 239, 4674, 83, 4675, 4676]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfl transport london mayor transport organisation much loved londoners'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleanup',\n",
       " 'effort',\n",
       " 'clapham',\n",
       " 'peopl',\n",
       " 'london',\n",
       " 'quickli',\n",
       " 'rebuild',\n",
       " 'restor',\n",
       " 'lives',\n",
       " 'great',\n",
       " 'photo']"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_stopwords_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-06 21:41:03,804 : INFO : collecting all words and their counts\n",
      "2020-04-06 21:41:03,805 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-04-06 21:41:03,806 : INFO : collected 65 word types from a corpus of 121 raw words and 20 sentences\n",
      "2020-04-06 21:41:03,807 : INFO : Loading a fresh vocabulary\n",
      "2020-04-06 21:41:03,808 : INFO : effective_min_count=5 retains 2 unique words (3% of original 65, drops 63)\n",
      "2020-04-06 21:41:03,809 : INFO : effective_min_count=5 leaves 22 word corpus (18% of original 121, drops 99)\n",
      "2020-04-06 21:41:03,810 : INFO : deleting the raw counts dictionary of 65 items\n",
      "2020-04-06 21:41:03,810 : INFO : sample=0.001 downsamples 2 most-common words\n",
      "2020-04-06 21:41:03,811 : INFO : downsampling leaves estimated 1 word corpus (4.5% of prior 22)\n",
      "2020-04-06 21:41:03,812 : INFO : estimated required memory for 2 words and 100 dimensions: 2600 bytes\n",
      "2020-04-06 21:41:03,812 : INFO : resetting layer weights\n",
      "2020-04-06 21:41:03,814 : INFO : training model with 3 workers on 2 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-06 21:41:03,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-06 21:41:03,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-06 21:41:03,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-06 21:41:03,822 : INFO : EPOCH - 1 : training on 121 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-04-06 21:41:03,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-06 21:41:03,828 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-06 21:41:03,828 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-06 21:41:03,829 : INFO : EPOCH - 2 : training on 121 raw words (1 effective words) took 0.0s, 280 effective words/s\n",
      "2020-04-06 21:41:03,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-06 21:41:03,833 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-06 21:41:03,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-06 21:41:03,835 : INFO : EPOCH - 3 : training on 121 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-04-06 21:41:03,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-06 21:41:03,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-06 21:41:03,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-06 21:41:03,841 : INFO : EPOCH - 4 : training on 121 raw words (2 effective words) took 0.0s, 568 effective words/s\n",
      "2020-04-06 21:41:03,844 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-06 21:41:03,846 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-06 21:41:03,847 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-06 21:41:03,847 : INFO : EPOCH - 5 : training on 121 raw words (1 effective words) took 0.0s, 269 effective words/s\n",
      "2020-04-06 21:41:03,848 : INFO : training on a 605 raw words (4 effective words) took 0.0s, 119 effective words/s\n",
      "2020-04-06 21:41:03,849 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-04-06 21:41:03,849 : INFO : storing 2x100 projection weights into tweets_notbinary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#Save word2vec format (not binary)\n",
    "\n",
    "model = Word2Vec(filtered_stopwords_list)\n",
    "print(model)\n",
    "model_save_location = \"tweets_notbinary\"\n",
    "model.wv.save_word2vec_format(model_save_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "london\n",
      "loot\n"
     ]
    }
   ],
   "source": [
    "#Word2vec load(2.option) example\n",
    "\n",
    "word2vec = {}\n",
    "with open('tweets_notbinary', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        print(word)\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test[1] : work top london model agenc call looker\n",
      "x_test_pad[1] : [   0    0    0    0    0    0    0    0    0    0    0   89  151    1\n",
      " 1583  668  133 6368]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test[1] :\",x_test[1])\n",
    "print(\"x_test_pad[1] :\",x_test_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6440"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(list(tokenizer.word_index))\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.uniform(-1, 0, (num_words+1, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6441, 100)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': array([100.], dtype=float32),\n",
       " 'london': array([ 3.8424553e-03, -2.2838530e-03, -3.0143522e-03,  2.4347934e-03,\n",
       "        -4.1387160e-03, -2.8542376e-03,  2.7846158e-03, -3.8115941e-03,\n",
       "        -4.6504592e-03, -2.7189122e-03, -2.0575700e-03, -4.7897738e-03,\n",
       "         1.4519703e-03,  5.8968546e-04,  4.1407091e-03,  1.6365096e-03,\n",
       "        -1.0659023e-03, -2.0151774e-03,  8.7965120e-05, -4.5869900e-03,\n",
       "         2.6919073e-03,  3.3096762e-03, -3.0899076e-03, -1.9432980e-03,\n",
       "        -2.9042671e-03, -1.6305504e-03, -3.2770068e-03,  4.2736498e-03,\n",
       "         3.1735075e-03,  8.3671766e-04,  1.4009308e-03, -8.0937799e-04,\n",
       "        -4.5718085e-03, -4.6393634e-03,  1.9156239e-04, -3.2500345e-03,\n",
       "         3.0848137e-03,  2.4407043e-03,  1.6724347e-05, -1.1905867e-03,\n",
       "        -4.0086107e-03, -2.1275415e-03,  1.5739833e-03, -3.6463456e-03,\n",
       "         4.4798041e-03, -3.4223683e-03,  5.3572573e-04, -3.9879109e-05,\n",
       "         3.6948377e-03, -2.0562937e-03,  1.8272239e-04,  1.7419485e-03,\n",
       "        -1.0752700e-03,  2.8849451e-04, -1.4333898e-03,  2.7398930e-03,\n",
       "        -2.6300899e-03,  8.6734782e-04, -1.8153380e-03, -1.4019993e-03,\n",
       "         6.9474615e-04,  2.5896963e-03, -8.0627488e-04, -3.8202666e-03,\n",
       "        -4.5247753e-03, -1.7061039e-03,  4.4873212e-03, -1.7731832e-03,\n",
       "         2.3817495e-03,  4.6297675e-03,  1.5058359e-03,  4.7848709e-03,\n",
       "         3.6658242e-03,  2.5801701e-03, -4.1704020e-03, -4.5118365e-03,\n",
       "        -4.0506246e-03,  4.7082938e-03, -4.1924729e-03,  8.0047158e-04,\n",
       "         2.3938394e-04, -2.7173024e-03, -6.1714940e-04,  3.4049642e-03,\n",
       "        -2.7530976e-03, -3.7765454e-03,  1.6301456e-03, -4.9023801e-03,\n",
       "        -9.7083976e-04, -4.9522324e-03, -1.2686435e-03, -2.1057791e-04,\n",
       "         1.3982473e-03,  2.5690019e-03, -1.0345670e-03, -3.2987092e-03,\n",
       "         3.3348983e-03, -3.6912770e-03, -4.6057994e-03, -9.2370244e-04],\n",
       "       dtype=float32),\n",
       " 'loot': array([ 3.0438651e-04, -1.8338436e-03, -1.7699304e-03,  2.1385208e-03,\n",
       "         9.4870286e-04, -2.3855285e-03,  3.0778518e-03, -4.5956848e-03,\n",
       "         3.3835336e-03, -5.5948313e-04, -1.8921366e-03, -2.8452077e-03,\n",
       "        -1.7495158e-03, -2.1879734e-03, -3.6747053e-03, -2.2203529e-03,\n",
       "        -1.6193449e-03, -2.0730590e-04, -2.9646340e-03, -3.0623721e-03,\n",
       "        -2.9524643e-04,  1.5608108e-03,  3.2647320e-03,  3.8934380e-03,\n",
       "        -2.4944122e-03, -2.7970113e-03, -3.0826952e-03,  3.4620999e-03,\n",
       "         3.9632423e-03, -2.7760421e-03, -4.7736582e-03,  5.6819076e-04,\n",
       "        -5.0239475e-04,  2.5273846e-03, -4.4785002e-03, -6.7673222e-04,\n",
       "         1.6335342e-03, -1.2979121e-03, -1.4092333e-03,  4.4328589e-03,\n",
       "        -2.1252630e-03, -2.0413974e-03, -3.6168257e-03, -3.2259650e-03,\n",
       "         1.0647764e-03,  2.7875623e-03,  3.2110724e-03,  2.1526413e-03,\n",
       "         1.4614691e-03,  3.6425560e-03, -1.8051296e-03, -9.7351731e-04,\n",
       "         1.5196896e-03,  1.7358674e-03, -3.8506533e-03,  4.8093074e-03,\n",
       "        -7.3911593e-04, -2.0340541e-03, -1.6456938e-03,  3.1561393e-03,\n",
       "        -9.5083182e-05, -1.3905629e-06,  1.8616288e-04,  2.3818014e-03,\n",
       "         5.7397858e-05, -4.4239173e-04,  5.2416606e-05,  2.8337007e-03,\n",
       "        -3.5468673e-03,  4.5110723e-03, -3.5399296e-03, -4.9595479e-03,\n",
       "         4.8571010e-03,  7.7497133e-04,  2.9903580e-03, -3.6863701e-03,\n",
       "        -4.1998695e-03,  1.1577919e-03,  3.5407647e-04, -2.6217843e-03,\n",
       "        -4.0342589e-03, -7.0628058e-04, -7.2781421e-04, -4.4675521e-03,\n",
       "        -4.8559480e-03, -4.7961954e-04,  4.7957553e-03, -3.6327685e-03,\n",
       "         3.2184615e-03, -6.2451040e-04,  2.9275597e-03, -4.6805539e-03,\n",
       "         3.8993149e-03, -1.6968857e-04,  3.0173799e-03,  2.2764695e-03,\n",
       "        -2.3692285e-03,  1.8358065e-03,  1.2855091e-03, -1.5403626e-03],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.76928148, -0.64693835, -0.7298864 , -0.99696975, -0.78386647,\n",
       "       -0.92315654, -0.1016227 , -0.3894247 , -0.92895017, -0.20991172,\n",
       "       -0.07065759, -0.0043379 , -0.13894763, -0.50358192, -0.84148005,\n",
       "       -0.88591407, -0.31443385, -0.81890978, -0.59805769, -0.53464038,\n",
       "       -0.00650707, -0.65098198, -0.24260558, -0.76650254, -0.42686459,\n",
       "       -0.50125806, -0.25816101, -0.92453469, -0.71996442, -0.70303242,\n",
       "       -0.80968557, -0.33692404, -0.82924931, -0.98574054, -0.3560607 ,\n",
       "       -0.33291955, -0.09805998, -0.98494714, -0.69112342, -0.35247695,\n",
       "       -0.7630395 , -0.14066357, -0.90932862, -0.24340056, -0.99005106,\n",
       "       -0.74964489, -0.73355949, -0.74776258, -0.91256499, -0.65965357,\n",
       "       -0.51164343, -0.79927932, -0.91905301, -0.60846795, -0.4467129 ,\n",
       "       -0.95687322, -0.9218611 , -0.35818776, -0.98712193, -0.31314223,\n",
       "       -0.00849346, -0.950322  , -0.58228955, -0.72777067, -0.8651078 ,\n",
       "       -0.4198136 , -0.53959316, -0.44939766, -0.94224139, -0.9524448 ,\n",
       "       -0.18242857, -0.25278724, -0.91354035, -0.62646887, -0.55175443,\n",
       "       -0.75807272, -0.18080352, -0.58856478, -0.56018789, -0.8474792 ,\n",
       "       -0.77721516, -0.60691294, -0.08450489, -0.70127355, -0.90721194,\n",
       "       -0.76667054, -0.67267008, -0.43457476, -0.71403371, -0.022823  ,\n",
       "       -0.45556384, -0.72803792, -0.31255038, -0.9617465 , -0.70855509,\n",
       "       -0.60905234, -0.57885349, -0.92789619, -0.121667  , -0.83209376])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = max_tokens\n",
    "vocabulary_size = num_words\n",
    "embedding_dim = embedding_size\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "epochs = 5\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = []\n",
    "y_test2 = []\n",
    "for i in y_train:\n",
    "  y_train2.append(int(i))\n",
    "for i in y_test:\n",
    "  y_test2.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 18, 100)           644100    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 18, 64)            44864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 9, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 9, 64)             28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 719,846\n",
      "Trainable params: 719,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2176 samples, validate on 242 samples\n",
      "Epoch 1/5\n",
      "2176/2176 - 1s - loss: 0.3925 - accuracy: 0.8617 - val_loss: 0.3949 - val_accuracy: 0.9215\n",
      "Epoch 2/5\n",
      "2176/2176 - 0s - loss: 0.3026 - accuracy: 0.9246 - val_loss: 0.2815 - val_accuracy: 0.9215\n",
      "Epoch 3/5\n",
      "2176/2176 - 0s - loss: 0.2772 - accuracy: 0.9265 - val_loss: 0.2857 - val_accuracy: 0.9215\n",
      "Epoch 4/5\n",
      "2176/2176 - 0s - loss: 0.2672 - accuracy: 0.9265 - val_loss: 0.2847 - val_accuracy: 0.9215\n",
      "Epoch 5/5\n",
      "2176/2176 - 0s - loss: 0.2620 - accuracy: 0.9265 - val_loss: 0.2802 - val_accuracy: 0.9215\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#Training params\n",
    "batch_size = 256 \n",
    "num_epochs = 5\n",
    "\n",
    "#Model parameters\n",
    "num_filters = 64  \n",
    "embed_dim = embedding_size\n",
    "weight_decay = 1e-5\n",
    "\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "\n",
    "#Model add word2vec embedding\n",
    "\n",
    "model.add(Embedding(input_dim=num_words+1,\n",
    "                    output_dim=embedding_size,\n",
    "                    weights= [embedding_matrix],\n",
    "                    input_length=max_tokens,        \n",
    "                    trainable=True,              #the layer is trained\n",
    "                    name='embedding_layer'))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "\n",
    "hist = model.fit(x_train_pad, y_train2, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2176 samples, validate on 242 samples\n",
      "Epoch 1/5\n",
      "2176/2176 - 0s - loss: 0.2581 - accuracy: 0.9265 - val_loss: 0.2812 - val_accuracy: 0.9215\n",
      "Epoch 2/5\n",
      "2176/2176 - 0s - loss: 0.2587 - accuracy: 0.9265 - val_loss: 0.2790 - val_accuracy: 0.9215\n",
      "Epoch 3/5\n",
      "2176/2176 - 0s - loss: 0.2489 - accuracy: 0.9265 - val_loss: 0.2701 - val_accuracy: 0.9215\n",
      "Epoch 4/5\n",
      "2176/2176 - 0s - loss: 0.2503 - accuracy: 0.9265 - val_loss: 0.2789 - val_accuracy: 0.9215\n",
      "Epoch 5/5\n",
      "2176/2176 - 0s - loss: 0.2337 - accuracy: 0.9265 - val_loss: 0.2696 - val_accuracy: 0.9215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x288c328a788>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train2, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incercaaaaam sa evaluuuaaaam modelul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   33, 2521, 1239],\n",
       "       [   0,    0,    0, ...,  668,  133, 6368],\n",
       "       [   0,    0,    0, ...,  447,   14,  117],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  271,    1, 2353],\n",
       "       [   0,    0,    0, ...,    6,    5,   29],\n",
       "       [   0,    0,    0, ..., 2443,  281,  206]])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "#Training params\n",
    "batch_size = 256 \n",
    "num_epochs = 5\n",
    "\n",
    "#Model parameters\n",
    "num_filters = 64  # görüntünün boyutu mesela 512*512\n",
    "embed_dim = embedding_size\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/605 [==============================] - 0s 39us/sample - loss: 0.2613 - accuracy: 0.9140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2612976095154266, 0.91404957]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pad, y_test2, batch_size, verbose=1, sample_weight=None, steps=None, callbacks=callbacks_list, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(x_train_pad, batch_size, verbose=0, steps=None, callbacks=callbacks_list, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05504617, 0.9449538 ], dtype=float32)"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Exportare rezultat ca csv\n",
    "with open('rezultaaaat.csv', mode='w') as cv:\n",
    "    scris = csv.writer(cv)\n",
    "    for index in range(len(result)):\n",
    "        scris.writerow([result[index][0], result[index][1] , y_train[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,  127,   60,  129, 1012,\n",
       "        959, 1012, 4284,  127,   60, 4285,    2])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unlabeled_tokens = tokenizer.texts_to_sequences(filtered_unlabeled_tweets)\n",
    "x_unlabeled_pad= pad_sequences(x_unlabeled_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1266,   24,    1,  159,  180],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,   61, 1138,  895],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0, 3380,  127,   60,\n",
       "        1460, 1343,    1,  127,   60, 1460, 1343],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  410,  392,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         581,  388,    1,   17,  924,  361,   57],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    3, 1232,   82,    1,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 1232,   82,    1,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    1, 1372,   82, 1623,  757,\n",
       "         209,  127, 5761, 1372, 2039,   78, 5761],\n",
       "       [   0,    0,    0,    0,    0,    0,    1, 1372,   82, 1623,  757,\n",
       "         209,  127, 5761, 1372, 2039,   78, 5761],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 1232,   82,    1,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 1232,   82,    1,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,   27,  122,  108,  783,  199],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   27,  122,  108,  783,  199],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    1,   57, 6380],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,  820,  412,\n",
       "         973,    1,  172,  125,  412,  820, 1637],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  705,    1,   70],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 1404,  768,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 1404,  768,    1]])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unlabeled_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(x_train_pad, batch_size, verbose=0, steps=None, callbacks=callbacks_list, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04235659, 0.95764345],\n",
       "       [0.05504617, 0.9449538 ],\n",
       "       [0.09127739, 0.9087226 ],\n",
       "       ...,\n",
       "       [0.10588056, 0.89411944],\n",
       "       [0.06583234, 0.9341677 ],\n",
       "       [0.04062131, 0.9593787 ]], dtype=float32)"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
